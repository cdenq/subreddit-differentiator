{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Introduction</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Foreword</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christopher Denq\n",
    "\n",
    "5/5/2023\n",
    "\n",
    "Notebook 1 of 3\n",
    "\n",
    "Contains the scrapper functions that are used to generate the data for the NLP project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Code Setup</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset & Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Setup\n",
    "url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "# subreddits = [\"lawschooladmissions\", \"gradadmissions\"]\n",
    "subreddits_2 = [\"AskReddit\", \"askscience\"] # switched to this subreddit due to lack of activity in previous subreddits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for checking data\n",
    "def check_size(subreddits: list) -> None:\n",
    "    \"\"\" \n",
    "    Prints the number of rows in the indicated databases.\n",
    "\n",
    "    Parameters:\n",
    "        subreddits: list\n",
    "            A list containing the subreddit names as string; used in the pushshift url\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "            Prints the number of rows in the subreddit database\n",
    "    \"\"\"\n",
    "    for subreddit in subreddits:\n",
    "        df = pd.read_csv(f\"../scrapped_data/{subreddit.lower()}_data.csv\", encoding=\"utf-8-sig\")\n",
    "        print(f\"/r/{subreddit.lower()} has {df.shape[0]} entries total\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for scrapping data\n",
    "def scrape(subreddits: list, mode: str=\"new\", intial: bool=False) -> None:\n",
    "    \"\"\" \n",
    "    Scrapes provided subreddits and automatically updates pre-existing database of reddit posts, if any. \n",
    "\n",
    "    Parameters:\n",
    "        subreddits: list\n",
    "            A list containing the subreddit names as string; used in the pushshift url\n",
    "\n",
    "        mode: str, default=\"new\"\n",
    "            \"new\" indicates scraping posts that have been generate after the earliest post in database\n",
    "            \"old\" indicates scraping posts that have been generate before the oldest post in database\n",
    "\n",
    "        intial: bool, default=\"False\"\n",
    "            \"True\" indicates that this is the first scrape (creating database)\n",
    "            \"False\" indicates that this is a subsequent scrape (updated database)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "            Appropriate database is either created or updated\n",
    "    \"\"\"\n",
    "    # Loop through subreddits to fetch data\n",
    "    for subreddit in subreddits:\n",
    "        print(f\"Starting /r/{subreddit.lower()}...\")\n",
    "\n",
    "        # Setup URL params and make connection\n",
    "        params = {\n",
    "            \"subreddit\": subreddit,\n",
    "            \"limit\": 1000,\n",
    "        }\n",
    "        if not intial: # Change params depending on whether this is our first extraction\n",
    "            existing_df = pd.read_csv(f\"../scrapped_data/{subreddit.lower()}_data.csv\", encoding=\"utf-8-sig\")\n",
    "            newest_time, oldest_time = existing_df[\"created_utc\"].values[-1], existing_df[\"created_utc\"].values[0]\n",
    "            if mode == \"new\":\n",
    "                params[\"after\"] = newest_time\n",
    "            elif mode == \"old\":\n",
    "                params[\"before\"] = oldest_time\n",
    "            else:\n",
    "                print('Please input \"old\" or \"new\" for the function.')\n",
    "                return\n",
    "        response = requests.get(url, params)\n",
    "\n",
    "        # Loading data\n",
    "        if response.status_code == 200:\n",
    "            print(\"Successful connection!\")\n",
    "            df = pd.DataFrame(response.json()['data'])[['subreddit', 'selftext', 'title', 'author_flair_text', 'created_utc', 'url']]\n",
    "        else:\n",
    "            print(\"Failed connection...\")\n",
    "            print(f\"Leaving /r/{subreddit.lower()}!\")\n",
    "            print(\"=\"*10)\n",
    "            continue\n",
    "\n",
    "        # Combining datasets\n",
    "        if mode == \"new\":\n",
    "            combined_df = pd.concat([existing_df, df], axis=0)\n",
    "        else:\n",
    "            combined_df = pd.concat([df, existing_df], axis=0)\n",
    "        combined_df = combined_df.sort_values(by=\"created_utc\", ascending=True)\n",
    "\n",
    "        # Save data to CSV\n",
    "        combined_df.to_csv(f\"../scrapped_data/{subreddit.lower()}_data.csv\", encoding=\"utf-8-sig\", index=False)\n",
    "        print(f\"Added {df.shape[0]} entries!\")\n",
    "        print(\"=\"*10)\n",
    "\n",
    "    # Output\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Code</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONLY IF FIRST TIME OR ELSE IT WILL WIPE YOUR DATASET\n",
    "# scrape(subreddits_2, #initial=True#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/r/askreddit has 13149 entries total\n",
      "/r/askscience has 12314 entries total\n"
     ]
    }
   ],
   "source": [
    "# scrape(subreddits_2, \"old\") # Scrape historical posts\n",
    "# scrape(subreddits_2) # Scrape new posts\n",
    "check_size(subreddits_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
